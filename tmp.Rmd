---
title: "Metro"
author: "Christian Barz"
date: "25 5 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
```

# Memory



# True data generating process

We generated data in a way one would do it in a lab, i.e. run an experiment multiple times.



```{r}
n <- 1
m <- 2
sigma <- 10.

# done a series of experiments and repeated it 4 times
x <- rep.int(x = seq(from = 0, to = 20, by = 1),
             times = 4)
y <- sapply(x, function(i){
  rnorm(n = 1,
        mean = m * i + n,
        sd = sigma)
})

# system matrix, design
df <- dplyr::tibble(x = x,
                    intercept = 1)
X <- as.matrix(df)
# model y = A*x, mit A hat hat eine konstante spalte 1 fÃ¼r den intercept
parameter <- c(m,n)
sd <- sigma
```




# model / theory

We assume that $y$ depends linear on $x$ and each measurement has a normaly distributed error, i.e.

$$
y_i = m\cdot x_i + n + \epsilon_i
$$

or equivalent

$$
y_i \propto N(m\cdot x_i+ n, \sigma)
$$


Using Bayes rule we estimate the distribution of $n,m$ by

$$
p(m,n|y,x) \propto p(y|n,n,x)\cdot p(m,n)
$$


Let us be more precise in order to see which additional assumtions we may need for the implementation:

$$
p(m,n|y_1=y_1,\ldots,y_n=y_n, x_1=x_1,\ldots,x_n=x_n) \propto \\
 p(y_1=y_1,\ldots,y_n=y_n|n,n,x_1=x_1,\ldots,x_n=x_n)\cdot\\
 p(m,n)
$$

Suppose the measurements are independently and identically distributed, then this simplifies to

$$
p(m,n|y,x) = p(m,n)\cdot \prod_{i=1}^n p(y_i|m,n,x_i)
$$

to avoid numerical underflow we consider and implement a logarithm


# implementation

## likelihood

```{r}
likelihood <- function(X,y, parameter, sd = 3){
  yhat <- X %*% parameter
 
  likelihood_per_data_point <- dnorm(y, 
                                     mean = yhat,
                                     sd = sd,
                                     log = TRUE)
  sum_of_likelihoods <- sum(likelihood_per_data_point)
  
  
  return(sum_of_likelihoods)
}
```

### plot likelihood




## prior

```{r}
prior <- function(parameter){
  log_prob_parameter <- dnorm(parameter, sd = 3, log = TRUE)
  sum(log_prob_parameter)
}
```


## posterior

```{r}
posterior <- function(parameter, X,y){
  likelihood(X = X, y = y, parameter = parameter) + prior(parameter)
}
```

## MH

**ingredients**

- proposal distribution
- random initial value

### proposal function

```{r}
proposal_function <- function(parameter){
  new_parameter <- parameter
  for (i in 1:NROW(parameter)) {
    new_parameter[i] <- rnorm(n = 1, mean = parameter[i], sd = 3)
  }
  
  new_parameter
}
```

### acceptance


```{r}
MH <- function(initial_value, iterations){
  chain <- array(dim = c(iterations+1, 2))
  
  chain[1,] <- initial_value
  for (i in 1:iterations) {
    
    candidate <- proposal_function(chain[i,])
    
    acceptance <- exp(posterior(X = X,
                                y= y,
                                parameter = candidate) - 
                        posterior(X = X, 
                                  y = y, 
                                  parameter = chain[i,])
                      )
   
    if(runif(1) < acceptance){
      chain[i+1,] <- candidate
    }else{
      chain[i+1,] <- chain[i,]
    }
  }
  return(chain)
}
```


## test

recall our true data generating process $y = m\cdot x + n$

we initialized with

```{r}
m
n
```


```{r}
startvalue = c(10,10)
iterations <- 400000
chain = MH(startvalue, iterations)
# discard warmup
warmup_length <- round(iterations/4, digits = 0)
chain <- chain[warmup_length:iterations,]
hist(chain[,1])
hist(chain[,2])
```


# diagnostics

```{r}
dt <- tibble(m = chain[,1], n = chain[,2]) %>%
  mutate(id = 1:nrow(.))
```

## traceplots

```{r}
dt %>%
  ggplot(aes(x=id, y = m)) + geom_point()
```

```{r}
dt %>%
  ggplot(aes(x=id, y = n)) + geom_point()
```
