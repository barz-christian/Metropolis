---
title: "Metropolis and Gibbs sampling"
author: "Christian Barz"
date: "25 5 2021"
output: 
  github_document:
    toc: true
    toc_depth: 2
    number_sections: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(dplyr)
library(ggplot2)
```

# Memo



# True data generating process

We generated data in a way one would do it in a lab, i.e. run an experiment multiple times.



```{r}
n <- 1
m <- 2
sigma <- 10.

# done a series of experiments and repeated it 4 times
x <- rep.int(x = seq(from = 0, to = 20, by = 1),
             times = 4)
y <- sapply(x, function(i){
  rnorm(n = 1,
        mean = m * i + n,
        sd = sigma)
})

# system matrix, design
df <- dplyr::tibble(x = x,
                    intercept = 1)
X <- as.matrix(df)
# model y = A*x, mit A hat hat eine konstante spalte 1 fÃ¼r den intercept
parameter <- c(m,n)
sd <- sigma
```




# model / theory

## model

We assume that our observation $y$ depends linear on $x$ and each measurement has a
normal distributed error, i.e. each observation can be written as

$$
\begin{equation}
y_i = m \cdot xi + n + \epsilon_i,
\end{equation}
$$

where $\epsilon_i\sim N(0,\sigma^2)$.

This can be written in the following equivalent form:

$$
\begin{equation}
y_i \sim N(m\cdot x_i+n, \sigma^2).
\end{equation}
$$

One benefit of the last notation is, that we directly see (the) 3 parameters of our model:

- $m$ the slope of the regression line,
- $n$ the intercept of the regression line,
- $\sigma^2$ the variance of the error term (corresponds to the amount of error of each measurement).

We do a Bayesian approach to infer the distribution of the regression parameters $n,m$. Hence we have to specify a prior on the parameters. 
For this example we use:

**!!Include PRIORS here!!**


This way we directly see the hyperparameters of the model.




## derivation of the formulas for the likelihood

We are going to derive step by step a term that is proportional to the joint posterior distribution of $n,m$ given $y$. 
But we do not only want to derive a formula; **no** we also want to derive the sufficient and necessary conditions for the formula, which are often not mentioned.

$$
\begin{align}
p(m,n|y) & =      & \frac{p(y|m,n) p(m,n)}{p(y)}\\ 
         &\propto & p(y|m,n)  p(m,n) \\
         &=       & p(y|m,n)  p(m) p(n) 
\end{align}
$$

Here the last line the last line requires that the parameters $m,n$ are statistically independent, i.e. the joint probability is equal to the product of the marginal distributions:

$$
\begin{equation*}
p(m,n)=p(m)p(n)
\end{equation*}
$$

In general the quantity $y$ is a matrix, but for simplicity we restrict to the case when $y=(y_1,\ldots,y_n)$ is a vector. In this case the posterior is proportional to

$$
\begin{align}
p(m,n|y)
&    =    p(m,n|y_1,\ldots,y_n)\\
&\propto  p(y_1,\ldots,y_n|m,n) p(m) p(n) \\
& =       \prod_{i=1}^{n} p(y_i|m,n) p(m) p(n)
\end{align}
$$

In the last step we used the assumption that the measurements are independently and identically distributed.

In order to avoid numerical underflow we apply the natural logarithm and get:

### log likelihood

$$
\begin{align}
\log p(y|m,n) &= \sum_{i=1}^{n} \log p(y_i|m,n)\\
              &= \sum \log p_{normal}(y_i-m \cdot x_i + n,\sigma^2)
\end{align}
$$

### log prior

### log posteriori

Because we have used conjugated priors we know the closed formof the posterior and can compare the true posterior with the approximation we get from our Metropolis implemenation.

# implementation

## likelihood

```{r}
likelihood <- function(X,y, parameter, sd = 3){
  yhat <- X %*% parameter
 
  likelihood_per_data_point <- dnorm(y, 
                                     mean = yhat,
                                     sd = sd,
                                     log = TRUE)
  sum_of_likelihoods <- sum(likelihood_per_data_point)
  
  
  return(sum_of_likelihoods)
}
```

### plot likelihood




## prior

```{r}
prior <- function(parameter){
  log_prob_parameter <- dnorm(parameter, sd = 3, log = TRUE)
  sum(log_prob_parameter)
}
```


## posterior

```{r}
posterior <- function(parameter, X,y){
  likelihood(X = X, y = y, parameter = parameter) + prior(parameter)
}
```

## MH

**ingredients**

- proposal distribution
- random initial value

### proposal function

```{r}
proposal_function <- function(parameter){
  new_parameter <- parameter
  for (i in 1:NROW(parameter)) {
    new_parameter[i] <- rnorm(n = 1, 
                              mean = parameter[i], 
                              sd = 1)
  }
  
  new_parameter
}
```

### acceptance


```{r}
MH <- function(initial_value, iterations){
  chain <- array(dim = c(iterations+1, 2))
  
  chain[1,] <- initial_value
  for (i in 1:iterations) {
    
    candidate <- proposal_function(chain[i,])
    
    acceptance <- exp(posterior(X = X,
                                y= y,
                                parameter = candidate) - 
                        posterior(X = X, 
                                  y = y, 
                                  parameter = chain[i,])
                      )
   
    if(runif(1) < acceptance){
      chain[i+1,] <- candidate
    }else{
      chain[i+1,] <- chain[i,]
    }
  }
  return(chain)
}
```


## test

recall our true data generating process $y = m\cdot x + n$ and the values of the parameters

```{r}
m
n
```


```{r run metropolis}
startvalue = c(10,10)
iterations <- 200
warmup_length <- round(iterations/10, digits = 0)

chain = MH(startvalue, iterations)

posterior_samples <- tibble(m = chain[,1],
             n = chain[,2]) %>%
  mutate(id = 1:nrow(.)) %>%
  mutate(warmup = ifelse(id <= warmup_length,TRUE,FALSE))

# plot parameters with warmup discarded
posterior_samples %>%
  filter(!warmup) %>%
  ggplot() + geom_histogram(aes(m))


posterior_samples %>%
  filter(!warmup) %>%
  ggplot() + geom_histogram(aes(n))
```


# diagnostics

For simplicity we restrict to visual checks, precisely to traceplots.


## traceplots warmup

Traceplot of the slope parameter `n`

```{r}
posterior_samples %>%
  filter(warmup) %>%
  ggplot(aes(x=id, y = m)) + geom_point()
```

Traceplot of the intercept parameter `n`

```{r}
posterior_samples %>%
  filter(warmup) %>%
  ggplot(aes(x=id, y = n)) + geom_point()
```

## traceplots warmup discarded

```{r}
posterior_samples %>%
  filter(!warmup) %>%
  ggplot(aes(x=id, y = m)) + geom_point()
```

```{r}
posterior_samples %>%
  filter(!warmup) %>%
  ggplot(aes(x=id, y = m)) + geom_line()
```


```{r}
posterior_samples %>%
  filter(!warmup) %>%
  ggplot(aes(x=id, y = n)) + geom_point()
```


```{r}
posterior_samples %>%
  filter(!warmup) %>%
  ggplot(aes(x=id, y = n)) + geom_line()
```